{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")  # Add the parent directory of LLM_Evaluations to the Python path\n",
    "from llm_evaluation_utils import load_responses_df, \\\n",
    "                        check_and_store_response,   \\\n",
    "                        build_question_prompt,      \\\n",
    "                        QUESTIONS\n",
    "\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "max_tokens = 250\n",
    "temperature = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined-Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp(timestamp):\n",
    "    \"\"\"Converts a Unix timestamp to a datetime object.\"\"\"\n",
    "    return datetime.fromtimestamp(timestamp) if timestamp is not None else None\n",
    "\n",
    "def retrieve_file_name(file_id, client: OpenAI):\n",
    "    \"\"\"Retrieve the filename associated with the given file ID.\"\"\"\n",
    "    try:\n",
    "        return client.files.retrieve(file_id).filename\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving file ID {file_id}:\", e)\n",
    "        return \"\"\n",
    "\n",
    "def upload_file(file_path, client: OpenAI):\n",
    "    \"\"\"Upload a JSONL file and return the file ID from OpenAI's server.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            file_upload_response = client.files.create(\n",
    "                file=file,\n",
    "                purpose=\"batch\"\n",
    "            )\n",
    "        return file_upload_response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def create_batch(input_file_id, client: OpenAI):\n",
    "    \"\"\"Create a batch request and return the response.\"\"\"\n",
    "    try:\n",
    "        batch_response = client.batches.create(\n",
    "            input_file_id=input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "        return batch_response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def check_batch(batch_id, client: OpenAI):\n",
    "    \"\"\"Retrieve batch information using the provided batch ID.\"\"\"\n",
    "    try:\n",
    "        return client.batches.retrieve(batch_id)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def cancel_batch(batch_id, client: OpenAI):\n",
    "    \"\"\"Cancel a batch with the provided batch ID and return the cancellation response.\"\"\"\n",
    "    try:\n",
    "        return client.batches.cancel(batch_id)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def extract_batch_num(filename):\n",
    "    \"\"\"\n",
    "    Extract the batch number from the filename. \n",
    "    Filename should ends with an integer before the extension.\n",
    "    Example: `prompts-batch_3.jsonl`\n",
    "    \"\"\"\n",
    "    match = re.search(r'(\\d+)\\.', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def prepare_response_file(input_file_name, response_files_dir, responses_file_name=\"responses.jsonl\"):\n",
    "    \"\"\"Generate a response file name based on the batch number.\"\"\"\n",
    "    batch_num = extract_batch_num(input_file_name)\n",
    "    if batch_num is not None:\n",
    "        responses_file_name = f\"{responses_file_name.split('.')[0]}-batch_{batch_num}.jsonl\"\n",
    "    responses_file_path = os.path.join(response_files_dir, responses_file_name)\n",
    "    return responses_file_path, responses_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "transcript = \"Cluster headaches are treated in a multi-pronged approach. One, we give medications to try to stop the cluster headaches when the patient has them. These include medications like triptans, which are similarly used in migraine. Then we do a mini-preventive or short-term prophylaxis treatment. This is to try to give some quicker short-term relief in trying to decrease the number of the headaches. Then we use a long-term medication to give a more long-term prevention of the headaches.\"\n",
    "\n",
    "# instruction = \" \".join([QUESTION_HEAD, QUESTIONS[3], question_tail])\n",
    "# message = [{\"role\": \"user\", \"content\": f\"Instruction {instruction}\\nTranscript: {transcript}\\nScore:\"}]\n",
    "\n",
    "system = \" \".join([QUESTION_HEAD, QUESTION_TAIL])\n",
    "user = f\"Question: {QUESTIONS[3]}\\nTranscript: {transcript}\"\n",
    "message = [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user}\n",
    "        ]\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=message,\n",
    "        temperature=1,      # Between 0 and 2\n",
    "        max_tokens=10,\n",
    "        # top_p=0.1,        # alter this or termperature, but not both.\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "except openai.APIError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = \"sjH6PE9-MTI\"\n",
    "transcript = \"The trigger finger is a condition that involves the tendons of the hand and the fingers. The tendons of the fingers pass through a series of pulleys, much like a fishing line passes through a series of guide wires on a fishing pole. These pulleys give us a mechanical advantage and give us a very strong grip. However, oftentimes the tendons can become inflamed and the pulleys can become thickened. This is what is referred to as a trigger finger. The symptoms of trigger finger consist of pain at the base of the fingers that is often associated with a nodule or a small bump that can be felt and it can oftentimes be associated with catching or locking of the finger. In very severe cases, this does require manual assistance to obtain full extension of the fingers. Trigger finger is often caused by such conditions as rheumatoid arthritis, gout, or diabetes, but oftentimes the cause is undetermined. If you suspect that you may be suffering from trigger finger, the first thing to do is begin taking anti-inflammatory medications such as Aleve or ibuprofen. This will assist with the pain and will also decrease the inflammation around the tendon. You may also try gentle stretching at home, which would consist of locking the affected finger together and slowly stretching it out to try to stretch the tendon and prevent it from locking or triggering. If this does not work, it is advised to see your local hand surgeon, at which point you may receive a corticosteroid injection into the area of the pain at the base of the palm. A corticosteroid injection is extremely effective at decreasing the pain and oftentimes eliminating a trigger finger. If a corticosteroid injection does not work, a small surgery is recommended. For more information, please visit your local hand surgeon or visit www.orlandohandsurgery.com.\"\n",
    "\n",
    "transcripts_dir = \"../../Getting_Transcripts\"\n",
    "transcripts_file_name = \"merged_filtered_videos_transcripts.csv\"\n",
    "responses_dir = \".\"\n",
    "\n",
    "responses_df = load_responses_df(transcripts_dir, transcripts_file_name, responses_dir, model_name)\n",
    "responses_df.drop(columns=[\"Transcript\"], inplace=True)\n",
    "responses_df[\"Video ID\"] = range(0, len(responses_df))\n",
    "\n",
    "responses_df.iloc[0, 1:16] = [5, 5, 5, 1, 1, 4, 3, 1, 2, 3, 1, 2, 1, 5, 5]     # Medical Expert 1 scores\n",
    "# responses_df.iloc[0, 1:16] = [5, 5, 5, 3, 2, 3, 3, 2, 2, 4, 1, 2, 4, 5, 4]     # Medical Expert 2 scores\n",
    "responses_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests_limit_per_minute = 500\n",
    "base_delay = 60.0 / requests_limit_per_minute\n",
    "\n",
    "temperatures = [1.8,\n",
    "                1.6,\n",
    "                1.4,\n",
    "                1.2,\n",
    "                1,\n",
    "                0.8,\n",
    "                0.6,\n",
    "                0.4,\n",
    "                0.2,\n",
    "]\n",
    "\n",
    "prompt_templates = {\n",
    "#     0: \"\"\"You are a medical expert. Rate the following Transcripts according to the given Question.\n",
    "# Question: {question}\n",
    "# {QUESTION_TAIL}\n",
    "# Transcript: {transcript}\"\"\",\n",
    "    0: \"\"\"{QUESTION_HEAD}\n",
    "Question: {question}\n",
    "{QUESTION_TAIL}\n",
    "Transcript: {transcript}\"\"\",\n",
    "\n",
    "#     1: \"\"\"{QUESTION_HEAD} {question} {QUESTION_TAIL}\n",
    "# Transcript: {transcript}\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index = 1\n",
    "for temperature in temperatures:\n",
    "\n",
    "    for template_key, prompt_template in prompt_templates.items():\n",
    "        print(\"Template\", template_key, \"| Temperature:\", temperature)\n",
    "        for question_num, question in enumerate(QUESTIONS, start=1):\n",
    "            prompt = prompt_template.format(QUESTION_TAIL=QUESTION_TAIL,\n",
    "                                             QUESTION_HEAD=QUESTION_HEAD,\n",
    "                                             question=question,\n",
    "                                             transcript=transcript)\n",
    "            \n",
    "            message = [\n",
    "                # {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            response = client.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=message,\n",
    "                    temperature=temperature,      # Between 0 and 2\n",
    "                    max_tokens=250,\n",
    "                    # top_p=0.1,        # alter this or termperature, but not both.\n",
    "                )            \n",
    "            check_and_store_response(response.choices[0].message.content, responses_df, index, question_num)\n",
    "            time.sleep(base_delay)\n",
    "        index += 1\n",
    "    index += 1\n",
    "\n",
    "number_of_rows = index\n",
    "# number_of_rows = len(temp_topk_pairs) * (len(prompt_templates.keys()) + 1)\n",
    "responses_df.iloc[:,1:16].head(number_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows = 16\n",
    "diff_df_abs = abs(responses_df.iloc[0, 1:16] - responses_df.iloc[0:number_of_rows, 1:16])\n",
    "diff_df = responses_df.iloc[0, 1:16] - responses_df.iloc[0:number_of_rows, 1:16]\n",
    "\n",
    "selected_data = diff_df.iloc[0:number_of_rows, 0:16]\n",
    "sum_column = responses_df.iloc[0:number_of_rows, 1:16].sum(axis=1)\n",
    "sum_column_diff = diff_df.iloc[0:number_of_rows, 1:16].sum(axis=1)\n",
    "sum_column_abs_diff = diff_df_abs.iloc[0:number_of_rows, 1:16].sum(axis=1)\n",
    "selected_data[\"Sum\"] = sum_column\n",
    "selected_data[\"Difference Sum\"] = sum_column_diff\n",
    "selected_data[\"Absolute Difference Sum\"] = sum_column_abs_diff\n",
    "\n",
    "selected_data.head(number_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data.to_csv('Different prompts and temperature responses with expert 1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def count_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return count_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return count_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n",
    "\n",
    "# count_tokens_from_messages(message, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
